{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Network Constrained Linear Optimal Power Flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPSA-GB can model the GB power system  by solving a network constrained Linear Optimal Power Flow (LOPF) problem. This notebook shows the example application of a future 3 day period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "src_path = os.environ.get('PROJECT_SRC')\n",
    "os.chdir(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypsa\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import data_reader_writer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the required inputs for the LOPF: the start, end and year of simulation, and the timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv files for import\n",
    "start = '2040-02-28 00:00:00'\n",
    "end = '2040-03-01 23:30:00'\n",
    "# year of simulation\n",
    "year = int(start[0:4])\n",
    "# time step as fraction of hour\n",
    "time_step = 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose from one of the National Grid Future Energy Scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 'Leading The Way'\n",
    "scenario = 'Consumer Transformation'\n",
    "# scenario = 'System Transformation'\n",
    "# scenario = 'Steady Progression'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a baseline year (from 2010-2020). The baseline year determines which historical load profile and weather dataset is used for the future year modelled. The National Grid FES modellers used 2012 as their baseline year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_baseline = 2012"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_reader_writer is a script written to read in data from the various sources and write csv files in the format required for populating a PyPSA network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:741: PerformanceWarning: Non-vectorized DateOffset being applied to Series or DatetimeIndex\n",
      "  warnings.warn(\n",
      "c:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\indexing.py:1667: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.obj[key] = value\n",
      "C:\\Users\\alyden\\OneDrive - University of Edinburgh\\Python\\PyPSA-GB v0.0.1\\PyPSA-GB\\interconnectors.py:206: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  df_FES = df_FES[~df_FES.Variable.str.contains('(TWh)')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crookston\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000002815F2C3B20>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alyden\\OneDrive - University of Edinburgh\\Python\\PyPSA-GB v0.0.1\\PyPSA-GB\\add_P2G.py:64: FutureWarning: Dropping invalid columns in DataFrameGroupBy.add is deprecated. In a future version, a TypeError will be raised. Before calling .add, select only columns which should be valid for the function.\n",
      "  df_P2G_year = df_P2G.groupby(df_P2G.bus).sum()[year]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alyden\\OneDrive - University of Edinburgh\\Python\\PyPSA-GB v0.0.1\\notebooks\\5b - LOPF Future.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alyden/OneDrive%20-%20University%20of%20Edinburgh/Python/PyPSA-GB%20v0.0.1/notebooks/5b%20-%20LOPF%20Future.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_reader_writer\u001b[39m.\u001b[39;49mdata_writer(start, end, time_step, year, demand_dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39meload\u001b[39;49m\u001b[39m'\u001b[39;49m, year_baseline\u001b[39m=\u001b[39;49myear_baseline,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alyden/OneDrive%20-%20University%20of%20Edinburgh/Python/PyPSA-GB%20v0.0.1/notebooks/5b%20-%20LOPF%20Future.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                scenario\u001b[39m=\u001b[39;49mscenario, FES\u001b[39m=\u001b[39;49m\u001b[39m2022\u001b[39;49m, merge_generators\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, scale_to_peak\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, P2G\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\OneDrive - University of Edinburgh\\Python\\PyPSA-GB v0.0.1\\PyPSA-GB\\data_reader_writer.py:93\u001b[0m, in \u001b[0;36mdata_writer\u001b[1;34m(start, end, time_step, year, demand_dataset, year_baseline, scenario, FES, merge_generators, scale_to_peak, marine_modify, marine_scenario, P2G)\u001b[0m\n\u001b[0;32m     90\u001b[0m     generators\u001b[39m.\u001b[39mmerge_generation_buses(year)\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m P2G \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     add_P2G\u001b[39m.\u001b[39;49madd_P2G(year, scenario)\n",
      "File \u001b[1;32m~\\OneDrive - University of Edinburgh\\Python\\PyPSA-GB v0.0.1\\PyPSA-GB\\add_P2G.py:64\u001b[0m, in \u001b[0;36madd_P2G\u001b[1;34m(year, scenario, path, replace)\u001b[0m\n\u001b[0;32m     62\u001b[0m df_P2G[\u001b[39m'\u001b[39m\u001b[39mbus\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_P2G\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m r: gsp_to_bus(r, df_gsp_data), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(df_P2G\u001b[39m.\u001b[39mgroupby(df_P2G\u001b[39m.\u001b[39mbus))\n\u001b[1;32m---> 64\u001b[0m df_P2G_year \u001b[39m=\u001b[39m df_P2G\u001b[39m.\u001b[39;49mgroupby(df_P2G\u001b[39m.\u001b[39;49mbus)\u001b[39m.\u001b[39;49msum()[year]\n\u001b[0;32m     66\u001b[0m p_available \u001b[39m=\u001b[39m pd_generators_p_max_pu\u001b[39m.\u001b[39mmultiply(pd_generators\u001b[39m.\u001b[39mloc[pd_generators\u001b[39m.\u001b[39mindex[pd_generators\u001b[39m.\u001b[39mcarrier\u001b[39m.\u001b[39misin(carrier_list)], \u001b[39m'\u001b[39m\u001b[39mp_nom\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     67\u001b[0m p_available_by_bus \u001b[39m=\u001b[39m p_available\u001b[39m.\u001b[39mgroupby(pd_generators\u001b[39m.\u001b[39mbus, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1827\u001b[0m, in \u001b[0;36mGroupBy.sum\u001b[1;34m(self, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[39m# If we are grouping on categoricals we want unobserved categories to\u001b[39;00m\n\u001b[0;32m   1824\u001b[0m \u001b[39m# return zero, rather than the default of NaN which the reindexing in\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m \u001b[39m# _agg_general() returns. GH #31422\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m \u001b[39mwith\u001b[39;00m com\u001b[39m.\u001b[39mtemp_setattr(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mobserved\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m-> 1827\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_agg_general(\n\u001b[0;32m   1828\u001b[0m         numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[0;32m   1829\u001b[0m         min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m   1830\u001b[0m         alias\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madd\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1831\u001b[0m         npfunc\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49msum,\n\u001b[0;32m   1832\u001b[0m     )\n\u001b[0;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_output(result, fill_value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1342\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc)\u001b[0m\n\u001b[0;32m   1330\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_agg_general\u001b[39m(\n\u001b[0;32m   1332\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1337\u001b[0m     npfunc: Callable,\n\u001b[0;32m   1338\u001b[0m ):\n\u001b[0;32m   1340\u001b[0m     \u001b[39mwith\u001b[39;00m group_selection_context(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1341\u001b[0m         \u001b[39m# try a cython aggregation if we can\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_agg_general(\n\u001b[0;32m   1343\u001b[0m             how\u001b[39m=\u001b[39;49malias,\n\u001b[0;32m   1344\u001b[0m             alt\u001b[39m=\u001b[39;49mnpfunc,\n\u001b[0;32m   1345\u001b[0m             numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[0;32m   1346\u001b[0m             min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[0;32m   1347\u001b[0m         )\n\u001b[0;32m   1348\u001b[0m         \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1094\u001b[0m, in \u001b[0;36mDataFrameGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_mgr) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m   1085\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1086\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDropping invalid columns in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhow\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis deprecated. In a future version, a TypeError will be raised. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m   1092\u001b[0m     )\n\u001b[1;32m-> 1094\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_agged_manager(new_mgr)\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1688\u001b[0m, in \u001b[0;36mDataFrameGroupBy._wrap_agged_manager\u001b[1;34m(self, mgr)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m_consolidate()\n\u001b[0;32m   1687\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mresult_index\n\u001b[0;32m   1689\u001b[0m     mgr\u001b[39m.\u001b[39mset_axis(\u001b[39m1\u001b[39m, index)\n\u001b[0;32m   1690\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_constructor(mgr)\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:932\u001b[0m, in \u001b[0;36mBaseGrouper.result_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    930\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult_index\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[0;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 932\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroupings[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mresult_index\u001b[39m.\u001b[39mrename(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames[\u001b[39m0\u001b[39m])\n\u001b[0;32m    934\u001b[0m     codes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreconstructed_codes\n\u001b[0;32m    935\u001b[0m     levels \u001b[39m=\u001b[39m [ping\u001b[39m.\u001b[39mresult_index \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:629\u001b[0m, in \u001b[0;36mGrouping.result_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(group_idx, CategoricalIndex)\n\u001b[0;32m    628\u001b[0m     \u001b[39mreturn\u001b[39;00m recode_from_groupby(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_all_grouper, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort, group_idx)\n\u001b[1;32m--> 629\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_index\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:636\u001b[0m, in \u001b[0;36mGrouping.group_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     \u001b[39m# _group_index is set in __init__ for MultiIndex cases\u001b[39;00m\n\u001b[0;32m    635\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_index\n\u001b[1;32m--> 636\u001b[0m uniques \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_arraylike\n\u001b[0;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m Index(uniques, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:620\u001b[0m, in \u001b[0;36mGrouping.group_arraylike\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m    615\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_arraylike\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m    616\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39m    Analogous to result_index, but holding an ArrayLike to ensure\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m    we can can retain ExtensionDtypes.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_codes_and_uniques[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:670\u001b[0m, in \u001b[0;36mGrouping._codes_and_uniques\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    669\u001b[0m         na_sentinel \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 670\u001b[0m     codes, uniques \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49mfactorize(\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouping_vector, sort\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sort, na_sentinel\u001b[39m=\u001b[39;49mna_sentinel\n\u001b[0;32m    672\u001b[0m     )\n\u001b[0;32m    673\u001b[0m \u001b[39mreturn\u001b[39;00m codes, uniques\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\algorithms.py:761\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    759\u001b[0m         na_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 761\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[0;32m    762\u001b[0m         values, na_sentinel\u001b[39m=\u001b[39;49mna_sentinel, size_hint\u001b[39m=\u001b[39;49msize_hint, na_value\u001b[39m=\u001b[39;49mna_value\n\u001b[0;32m    763\u001b[0m     )\n\u001b[0;32m    765\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    766\u001b[0m     uniques, codes \u001b[39m=\u001b[39m safe_sort(\n\u001b[0;32m    767\u001b[0m         uniques, codes, na_sentinel\u001b[39m=\u001b[39mna_sentinel, assume_unique\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verify\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alyden\\Anaconda3\\envs\\PyPSA-GB\\lib\\site-packages\\pandas\\core\\algorithms.py:563\u001b[0m, in \u001b[0;36mfactorize_array\u001b[1;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    560\u001b[0m hash_klass, values \u001b[39m=\u001b[39m get_data_algo(values)\n\u001b[0;32m    562\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[1;32m--> 563\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39;49mfactorize(\n\u001b[0;32m    564\u001b[0m     values, na_sentinel\u001b[39m=\u001b[39;49mna_sentinel, na_value\u001b[39m=\u001b[39;49mna_value, mask\u001b[39m=\u001b[39;49mmask\n\u001b[0;32m    565\u001b[0m )\n\u001b[0;32m    567\u001b[0m codes \u001b[39m=\u001b[39m ensure_platform_int(codes)\n\u001b[0;32m    568\u001b[0m \u001b[39mreturn\u001b[39;00m codes, uniques\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5396\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5310\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "data_reader_writer.data_writer(start, end, time_step, year, demand_dataset='eload', year_baseline=year_baseline,\n",
    "                               scenario=scenario, FES=2022, merge_generators=True, scale_to_peak=True, P2G=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pypsa.Network()\n",
    "\n",
    "network.import_from_csv_folder('LOPF_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines need to be scaled up to accomadate for future generation, and specific analysis will be done on this in a later notebook.\n",
    "Note: interconnects are links in future, so don't need to be selective here (as was required in historical simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_factor = 4\n",
    "network.lines.s_max_pu *= contingency_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.consistency_check()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.lopf(network.snapshots, solver_name=\"gurobi\", pyomo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power output by generation type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the generators by the carrier, and print their summed power outputs over the simulation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_by_carrier = network.generators_t.p.groupby(\n",
    "    network.generators.carrier, axis=1).sum()\n",
    "\n",
    "storage_by_carrier = network.storage_units_t.p.groupby(\n",
    "    network.storage_units.carrier, axis=1).sum()\n",
    "\n",
    "# to show on graph set the negative storage values to zero\n",
    "storage_by_carrier[storage_by_carrier < 0] = 0\n",
    "\n",
    "p_by_carrier = pd.concat([p_by_carrier, storage_by_carrier], axis=1)\n",
    "\n",
    "print(network.links_t.p0)\n",
    "imp = network.links_t.p0.copy()\n",
    "imp[imp < 0] = 0\n",
    "imp['Interconnectors Import'] = imp.sum(axis=1)\n",
    "interconnector_import = imp[['Interconnectors Import']]\n",
    "print(interconnector_import)\n",
    "\n",
    "p_by_carrier = pd.concat([p_by_carrier, interconnector_import], axis=1)\n",
    "\n",
    "exp = network.links_t.p0.copy()\n",
    "exp[exp > 0] = 0\n",
    "exp['Interconnectors Export'] = exp.sum(axis=1)\n",
    "interconnector_export = exp[['Interconnectors Export']]\n",
    "print(interconnector_export)\n",
    "\n",
    "# group biomass stuff\n",
    "p_by_carrier['Biomass'] = (\n",
    "    p_by_carrier['Biomass (dedicated)'] + p_by_carrier['Biomass (co-firing)'])\n",
    "\n",
    "# rename the hydro bit\n",
    "p_by_carrier = p_by_carrier.rename(\n",
    "    columns={'Large Hydro': 'Hydro'})\n",
    "p_by_carrier = p_by_carrier.rename(\n",
    "    columns={'Interconnector': 'Interconnectors Import'})\n",
    "\n",
    "p_by_carrier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph the power output of the different generation types..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Nuclear\", 'Biomass',\n",
    "        'Waste', \"Oil\", \"Natural Gas\",\n",
    "        'Hydrogen', 'CCS Gas', 'CCS Biomass',\n",
    "        \"Pumped Storage Hydroelectric\", 'Hydro',\n",
    "        'Battery', 'Compressed Air', 'Liquid Air',\n",
    "        \"Wind Offshore\", 'Wind Onshore', 'Solar Photovoltaics',\n",
    "        'Interconnectors Import', 'Unmet Load'\n",
    "        ]\n",
    "\n",
    "p_by_carrier = p_by_carrier[cols]\n",
    "\n",
    "p_by_carrier.drop(\n",
    "    (p_by_carrier.max()[p_by_carrier.max() < 50.0]).index,\n",
    "    axis=1, inplace=True)\n",
    "\n",
    "\n",
    "colors = {\"Coal\": \"grey\",\n",
    "          \"Diesel/Gas oil\": \"black\",\n",
    "          \"Diesel/gas Diesel/Gas oil\": \"black\",\n",
    "          'Oil': 'black',\n",
    "          'Unmet Load': 'black',\n",
    "          'Anaerobic Digestion': 'green',\n",
    "          'Waste': 'chocolate',\n",
    "          'Sewage Sludge Digestion': 'green',\n",
    "          'Landfill Gas': 'green',\n",
    "          'Biomass (dedicated)': 'green',\n",
    "          'Biomass (co-firing)': 'green',\n",
    "          'Biomass': 'green',\n",
    "          'CCS Biomass': 'darkgreen',\n",
    "          'Interconnectors Import': 'pink',\n",
    "          \"Sour gas\": \"lightcoral\",\n",
    "          \"Natural Gas\": \"lightcoral\",\n",
    "          'CCS Gas': \"lightcoral\",\n",
    "          'Hydrogen': \"lightcoral\",\n",
    "          \"Nuclear\": \"orange\",\n",
    "          'Shoreline Wave': 'aqua',\n",
    "          'Tidal Barrage and Tidal Stream': 'aqua',\n",
    "          'Hydro': \"turquoise\",\n",
    "          \"Large Hydro\": \"turquoise\",\n",
    "          \"Small Hydro\": \"turquoise\",\n",
    "          \"Pumped Storage Hydroelectric\": \"darkturquoise\",\n",
    "          'Battery': 'lime',\n",
    "          'Compressed Air': 'greenyellow',\n",
    "          'Liquid Air': 'lawngreen',\n",
    "          \"Wind Offshore\": \"lightskyblue\",\n",
    "          'Wind Onshore': 'deepskyblue',\n",
    "          'Solar Photovoltaics': 'yellow'}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(15,10)\n",
    "(p_by_carrier / 1e3).plot(\n",
    "    kind=\"area\", ax=ax, linewidth=0,\n",
    "    color=[colors[col] for col in p_by_carrier.columns])\n",
    "\n",
    "# stacked area plot of negative values, prepend column names with '_' such that they don't appear in the legend\n",
    "(interconnector_export / 1e3).plot.area(ax=ax, stacked=True, linewidth=0.)\n",
    "# rescale the y axis\n",
    "ax.set_ylim([(interconnector_export / 1e3).sum(axis=1).min(), (p_by_carrier / 1e3).sum(axis=1).max()])\n",
    "\n",
    "# Shrink current axis's height by 10% on the bottom\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.9])\n",
    "\n",
    "# Put a legend below current axis\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "ax.set_ylabel(\"GW\")\n",
    "\n",
    "ax.set_xlabel(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph the pumped hydro dispatch and state of charge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(15,10)\n",
    "\n",
    "p_storage = network.storage_units_t.p.sum(axis=1)\n",
    "state_of_charge = network.storage_units_t.state_of_charge.sum(axis=1)\n",
    "p_storage.plot(label=\"Pumped hydro dispatch\", ax=ax, linewidth=3)\n",
    "state_of_charge.plot(label=\"State of charge\", ax=ax, linewidth=3)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"MWh\")\n",
    "ax.set_xlabel(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting line loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the line loading stats and graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = network.snapshots[60]\n",
    "\n",
    "print(\"With the linear load flow, there is the following per unit loading:\")\n",
    "loading = network.lines_t.p0.loc[now] / network.lines.s_nom\n",
    "loading.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "fig.set_size_inches(15, 17)\n",
    "\n",
    "network.plot(ax=ax, line_colors=abs(loading), line_cmap=plt.cm.jet, title=\"Line loading\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting locational marginal prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "fig.set_size_inches(20, 10)\n",
    "\n",
    "network.plot(ax=ax, line_widths=pd.Series(0.5, network.lines.index))\n",
    "plt.hexbin(network.buses.x, network.buses.y,\n",
    "           gridsize=20,\n",
    "           C=network.buses_t.marginal_price.loc[now],\n",
    "           cmap=plt.cm.jet)\n",
    "\n",
    "# for some reason the colorbar only works with graphs plt.plot\n",
    "# and must be attached plt.colorbar\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Locational Marginal Price (£/MWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.buses_t.marginal_price"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier = \"Wind Onshore\"\n",
    "\n",
    "capacity = network.generators.groupby(\"carrier\").sum().at[carrier, \"p_nom\"]\n",
    "p_available = network.generators_t.p_max_pu.multiply(network.generators[\"p_nom\"])\n",
    "p_available_by_carrier = p_available.groupby(network.generators.carrier, axis=1).sum()\n",
    "p_curtailed_by_carrier = p_available_by_carrier - p_by_carrier\n",
    "p_df = pd.DataFrame({carrier + \" available\": p_available_by_carrier[carrier],\n",
    "                     carrier + \" dispatched\": p_by_carrier[carrier],\n",
    "                     carrier + \" curtailed\": p_curtailed_by_carrier[carrier]})\n",
    "\n",
    "p_df[carrier + \" capacity\"] = capacity\n",
    "p_df[\"Wind Onshore curtailed\"][p_df[\"Wind Onshore curtailed\"] < 0.] = 0.\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(15,10)\n",
    "p_df[[carrier + \" dispatched\", carrier + \" curtailed\"]].plot(kind=\"area\", ax=ax, linewidth=0)\n",
    "# p_df[[carrier + \" available\", carrier + \" capacity\"]].plot(ax=ax, linewidth=0)\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Power [MW]\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('PyPSA-GB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a362b17abcd2c031329e78f98409aa17fadf72082aebdc1d10da213122c16a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
